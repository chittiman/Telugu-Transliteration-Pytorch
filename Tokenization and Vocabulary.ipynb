{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scripts.transliteration_tokenizers import create_source_target_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = Path.cwd()\n",
    "data_dir = cur_dir / \"data\"\n",
    "raw_data_dir = data_dir / \"raw_data\"\n",
    "proc_data_dir = data_dir / \"processed_data\"\n",
    "\n",
    "sample_file = raw_data_dir / \"sample.tsv\"\n",
    "\n",
    "train_file = proc_data_dir / \"train_clean.tsv\"\n",
    "dev_file = raw_data_dir / \"te.translit.sampled.dev.tsv\"\n",
    "test_file = raw_data_dir / \"te.translit.sampled.test.tsv\"\n",
    "\n",
    "weighted_sample_file = proc_data_dir / \"weighted_sample.tsv\"\n",
    "max_sample_file = proc_data_dir / \"max_sample.tsv\"\n",
    "repeat_sample_file = proc_data_dir / \"repeat_sample.tsv\"\n",
    "\n",
    "weighted_dev_file = proc_data_dir / \"weighted_dev.tsv\"\n",
    "max_dev_file = proc_data_dir / \"max_dev.tsv\"\n",
    "repeat_dev_file = proc_data_dir / \"repeat_dev.tsv\"\n",
    "\n",
    "weighted_train_file = proc_data_dir / \"weighted_train.tsv\"\n",
    "max_train_file = proc_data_dir / \"max_train.tsv\"\n",
    "repeat_train_file = proc_data_dir / \"repeat_train.tsv\"\n",
    "\n",
    "weighted_test_file = proc_data_dir / \"weighted_test.tsv\"\n",
    "max_test_file = proc_data_dir / \"max_test.tsv\"\n",
    "repeat_test_file = proc_data_dir / \"repeat_test.tsv\"\n",
    "\n",
    "tgt_corpus_file =  proc_data_dir / \"target_corpus.txt\"\n",
    "src_corpus_file = proc_data_dir / \"source_corpus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(src_corpus_file, 'r',encoding='utf-8') as file:\n",
    "    src_corpus = file.read()\n",
    "    \n",
    "with open(tgt_corpus_file, 'r',encoding='utf-8') as file:\n",
    "    tgt_corpus = file.read()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_corpus_keys =set(Counter(src_corpus.replace(\" \",'')).keys())\n",
    "print(sorted(src_corpus_keys))\n",
    "len(src_corpus_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ం', 'ః', 'అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'క', 'ఖ', 'గ', 'ఘ', 'చ', 'ఛ', 'జ', 'ఝ', 'ఞ', 'ట', 'ఠ', 'డ', 'ఢ', 'ణ', 'త', 'థ', 'ద', 'ధ', 'న', 'ప', 'ఫ', 'బ', 'భ', 'మ', 'య', 'ర', 'ఱ', 'ల', 'ళ', 'వ', 'శ', 'ష', 'స', 'హ', 'ా', 'ి', 'ీ', 'ు', 'ూ', 'ృ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', '్']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_corpus_keys =set(Counter(tgt_corpus.replace(\" \",'')).keys())\n",
    "print(sorted(tgt_corpus_keys))\n",
    "len(tgt_corpus_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The minimum number of tokens must be number of unique tokens + 4(one each for start, end, unk and pad tokens). Even if we use lesser number of tokens, it still defaults to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 66)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size = 24\n",
    "tgt_vocab_size = 24\n",
    "\n",
    "src_tokenizer, tgt_tokenizer = create_source_target_tokenizers(src_corpus_file,tgt_corpus_file, src_vocab_size,tgt_vocab_size)\n",
    "src_tokenizer.get_vocab_size(),tgt_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size = 100\n",
    "tgt_vocab_size = 100\n",
    "\n",
    "src_tokenizer, tgt_tokenizer = create_source_target_tokenizers(src_corpus_file,tgt_corpus_file, src_vocab_size,tgt_vocab_size)\n",
    "src_tokenizer.get_vocab_size(),tgt_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['sh', 'ak', 'ina', 'ra', 'vi', 'ani', 'du', 'bh', 'ag', 'aku', 'sth', 'aan', 'aru', 'ram', 'ti', 'ar', '</s>', 'lo', 'el', 'adh', 'm', 'd', 'p', 'oo', 'v', 'dh', 'ay', 't', 'l', 's', 'aal', 'ae', '<unk>', 'b', 'as', 'uu', 'unn', 'o', 'sam', 'alu', 'pra', 'w', 'at', 'av', '<s>', 'di', 'c', 'gaa', 'un', 'st', 'ath', 'ah', 'che', 'ari', 'ik', 'x', 'th', 'inch', 'i', 'ri', 'pr', 'a', 'y', '<pad>', 'f', 'q', 'al', 'en', 'on', 'z', 'aay', 'g', 'ulu', 'ni', 'ul', 'in', 'aar', 'ap', 'ki', 'im', 'it', 'u', 'uk', 'ru', 'il', 'e', 'r', 'aa', 'anu', 'ad', 'ut', 'j', 'aaru', 'ee', 'h', 'k', 'an', 'am', 'ch', 'n'])\n",
      "\n",
      "\n",
      "dict_keys(['ై', 'ఆ', 'థ', 'ఋ', 'కు', 'ల', 'పు', 'అ', 'తు', 'ె', 'ఢ', 'ా', 'ిం', 'న్న', 'లు', 'ది', '<pad>', 'ఞ', 'ు', 'ప', 'ార', 'ద', 'వ', 'డ', 'శ', 'ఐ', 'ో', 'ష', 'ప్ర', 'ల్', 'ఈ', 'ఏ', 'ఔ', 'ఝ', 'ర', 'డు', 'రా', 'క', '</s>', 'ఛ', 'ఠ', 'ి', 'న్', 'ఓ', 'ొ', 'రి', 'రు', '<s>', 'చ', 'మ', '్', 'ృ', 'ించ', 'గా', 'లో', 'ట్', 'ారు', 'ట', 'ఉ', 'ఒ', 'ను', 'ఫ', 'ర్', '్య', 'కి', 'వి', 'త్', 'ణ', 'ూ', 'ౌ', 'ని', 'తి', 'త', 'భ', 'ఱ', 'ాల', 'క్', 'ాయ', 'ః', 'జ', 'ీ', 'బ', 'ఊ', 'య', 'ే', '్ర', 'ము', 'ఎ', 'న', 'ఇ', 'గ', 'ధ', 'ళ', 'స', 'ఖ', 'ఘ', 'స్', 'హ', '<unk>', 'ం'])\n"
     ]
    }
   ],
   "source": [
    "print(src_tokenizer.get_vocab().keys())\n",
    "print(\"\\n\")\n",
    "print(tgt_tokenizer.get_vocab().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 66)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size = 30\n",
    "tgt_vocab_size = 66\n",
    "\n",
    "src_tokenizer, tgt_tokenizer = create_source_target_tokenizers(src_corpus_file,tgt_corpus_file, src_vocab_size,tgt_vocab_size)\n",
    "src_tokenizer.get_vocab_size(),tgt_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer_keys = set(src_tokenizer.get_vocab().keys())\n",
    "tgt_tokenizer_keys = set(tgt_tokenizer.get_vocab().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'</s>', '<pad>', '<s>', '<unk>'}, set())"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenizer_keys^src_corpus_keys,src_corpus_keys-src_tokenizer_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'</s>', '<pad>', '<s>', '<unk>'}, set())"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokenizer_keys^tgt_corpus_keys,tgt_corpus_keys-tgt_tokenizer_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, tokenizer keys are a superset of corpus keys at those given values and only additional elements in them are start,end,unk and pad tokens. So, at the given values it acts like a character level tokenizer and for values higher than that it acts like a byte pair tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>అంక</td>\n",
       "      <td>amka</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>అంక</td>\n",
       "      <td>anka</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>అంకం</td>\n",
       "      <td>amkam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>అంకం</td>\n",
       "      <td>ankam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>అంగీకరించ</td>\n",
       "      <td>amgiikarimcha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target         source  frequency\n",
       "0        అంక           amka          1\n",
       "1        అంక           anka          3\n",
       "2       అంకం          amkam          1\n",
       "3       అంకం          ankam          2\n",
       "4  అంగీకరించ  amgiikarimcha          1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample= pd.read_csv(sample_file, sep='\\t',header = None , names=[\"target\",\"source\", \"frequency\"])\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_encoding = src_tokenizer.encode(df_sample.source[0])\n",
    "src_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 4, 16, 14, 4, 2], ['<s>', 'a', 'm', 'k', 'a', '</s>'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_encoding.ids,src_encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'a', 'n', 'k', 'a', 'm', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'a', 'n', 'g', 'e', 'e', 'k', 'a', 'r', 'i', 'n', 'c', 'h', 'a', 'd', 'a', 'm', '</s>', '<pad>', '<pad>']\n",
      "['<s>', 'a', 'n', 'g', 'i', 'k', 'a', 'r', 'i', 'n', 'c', 'h', 'a', 'n', 'i', '</s>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'a', 'n', 'g', 'i', 'k', 'a', 'r', 'i', 'n', 'c', 'h', 'a', 'd', 'a', 'n', 'i', 'k', 'i', '</s>']\n",
      "['<s>', 'a', 'n', 't', 'h', 'a', 'm', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "for encoding in src_tokenizer.encode_batch(df_sample.sample(5).source.tolist()):\n",
    "    print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_encoding = tgt_tokenizer.encode(df_sample.target[0])\n",
    "tgt_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 6, 4, 19, 2], ['<s>', 'అ', 'ం', 'క', '</s>'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_encoding.ids,tgt_encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'న', 'ి', '</s>', '<pad>', '<pad>']\n",
      "['<s>', 'అ', 'ం', 'త', 'మ', 'ు', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'ా', 'ర', 'ు', '</s>', '<pad>']\n",
      "['<s>', 'అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', '</s>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', 'అ', 'ం', 'గ', 'ీ', 'క', 'ర', 'ి', 'ం', 'చ', 'ి', 'ం', 'ద', 'ి', '</s>']\n"
     ]
    }
   ],
   "source": [
    "for encoding in tgt_tokenizer.encode_batch(df_sample.sample(5).target.tolist()):\n",
    "    print(encoding.tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
